### YAML DEFAULT Config-File

#name of the model
name: 'AlphaZero'
#the config version should be updated whenever the old config looses compatibility,
#i.e. a change is made in the architecture of the model; new parameters
version: 1
#revision is changed whenever the parameters are reconfigured
#the model then gets a new unique name and a new folder for checkpoints, etc.
revision: 0
#directories
dirs:
    base: "_Data"
    checkpoint_dir: "model_checkpoints"
    game_dir: "games"
    dataset_dir: "datasets"
#player parameters
player:
    heat: 1
    exploration: 1
    rollout_payoffs:
        white:
            white_wins: 1
            black_wins: -1
            draw: 0
            draw_by_rep: 0
            draw_by_stale_mate: 0
            draw_by_two_wins: 0
        black: 
            white_wins: -1
            black_wins: 1
            draw: 0
            draw_by_rep: 0
            draw_by_stale_mate: 0
            draw_by_two_wins: 0
#model parameters
model:
    #input feature dimensions
    input_shape: [8,8,11]
    #total number of residual blocks
    resnet_depth: 9
    #residual block specifications
    residual_block:
        #number of layers in residual block
        layers: 2
        #number of filters per layer
        num_filters: 128
        #convolutional filter size: SxS
        filter_size: 3
        #stride of convolutional layer
        filter_stride: 1
        #activation function after conv layer
        activation: 'relu'
        #add batch normalization
        batch_normalization: True
    policy_head:
        #policy head's residual layer specifications
        residual_layer:
            num_filters: 192
            filter_size: 3
            filter_stride: 1
            batch_normalization: True
        #policy head's fully connected layer specifications
        dense_layer:
            #number of output filters for dense layer
            #this is also the number of valid moves (8x8xnum_filters)
            num_filters: 64
            #type of activation function
            activation: 'relu'
    value_head:
        #value head's residual layer specifications
        residual_layer:
            num_filters: 4
            filter_size: 3
            filter_stride: 1
            batch_normalization: True
        #value head's fully connected layer specifications
        dense_layer:
            #number of output filters for first dense layer
            #After Flatten()! Downsampling from 8x8xres_num_filt -> num_filters
            num_filters: 256
            #type of activation function for second dense layer
            activation: 'tanh'


# # # Leela Zero Config # # # 
#model:
#    input_shape: [8,8,64?]
#    resnet_depth: 15
#    residual_block:
#        layers: 2
#        filters: 192
#        filter_size: 3
#        filter_stride: 1
#        batch_normalization: True
#    ...
# # # Alpha Zero Config # # # 
#model:
#    input_shape: [8,8,73]
#    resnet_depth: 17
#    residual_block:
#        layers: 2
#        filters: 256
#        filter_size: 3
#        filter_stride: 1
#        batch_normalization: True
#    ...
# # # Alpha Go Config # # # 
#model:
#    input_shape: [8,8,?]
#    resnet_depth: 40
#    residual_block:
#        layers: 2
#        filters: 256
#        filter_size: 3
#        filter_stride: 1
#        batch_normalization: True
#    ...
